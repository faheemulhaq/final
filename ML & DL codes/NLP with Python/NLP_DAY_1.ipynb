{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d291ae",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "170346f0",
   "metadata": {},
   "source": [
    "Text preprocessing in NLP is essential to clean and transform raw text data for analysis. It involves removing noise, normalizing text (lowercasing, stemming, lemmatization), tokenization, handling stop words, contractions, and special characters, spell checking, and converting text to numerical representations. This process enhances data quality, ensures consistency, and facilitates effective feature extraction and model training in NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2930ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Text mining is the process of extracting valuable information from unstructured text data.\n",
      "\n",
      "Processed Text:\n",
      "['text', 'mining', 'process', 'extract', 'valuable', 'information', 'unstructured', 'text', 'datum', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and lemmatization\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Text mining is the process of extracting valuable information from unstructured text data.\"\n",
    "\n",
    "processed_text = preprocess_text(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nProcessed Text:\")\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9db1f2",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "397f7088",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into smaller units called tokens (words or subwords). Its significance lies in building vocabularies, facilitating text analysis, aiding feature extraction for machine learning, enabling text preprocessing, and contributing to language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88525f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Tokenization is crucial for natural language processing tasks.\n",
      "\n",
      "Tokenized Text:\n",
      "['Tokenization', 'is', 'crucial', 'for', 'natural', 'language', 'processing', 'tasks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Tokenization is crucial for natural language processing tasks.\"\n",
    "\n",
    "tokenized_text = tokenize_text(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nTokenized Text:\")\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa0062",
   "metadata": {},
   "source": [
    "# 3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d79b9ecb",
   "metadata": {},
   "source": [
    "Stemming: Simplifies words to their base form by removing suffixes, can result in non-words.\n",
    "    \n",
    "Lemmatization: Derives valid words based on context, ensuring precision and grammatical correctness.\n",
    "When to Choose:\n",
    "\n",
    "Stemming: Opt for speed and accept some loss of precision, suitable for search engines.\n",
    "    \n",
    "Lemmatization: Choose for accuracy and valid words, crucial in language understanding or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c308cf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Stemming and lemmatization are important for NLP tasks.\n",
      "\n",
      "Stemmed Text:\n",
      "['stem', 'and', 'lemmat', 'are', 'import', 'for', 'nlp', 'task', '.']\n",
      "\n",
      "Lemmatized Text:\n",
      "['Stemming', 'and', 'lemmatization', 'are', 'important', 'for', 'NLP', 'task', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def stem_text(text):\n",
    "    porter = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [porter.stem(word) for word in tokens]\n",
    "    return stemmed_words\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_words\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Stemming and lemmatization are important for NLP tasks.\"\n",
    "\n",
    "stemmed_text = stem_text(raw_text)\n",
    "lemmatized_text = lemmatize_text(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nStemmed Text:\")\n",
    "print(stemmed_text)\n",
    "\n",
    "print(\"\\nLemmatized Text:\")\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5e587",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ed353ad",
   "metadata": {},
   "source": [
    "Definition: Common words (e.g., \"the,\" \"and,\" \"is\") that are often removed during text preprocessing.\n",
    "Role: Eliminate noise, reduce dimensionality, and focus on more meaningful words.\n",
    "Impact: Enhance computational efficiency, improve model performance by prioritizing content-bearing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba22dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Stop words like 'the' and 'is' should be removed for better analysis.\n",
      "\n",
      "Text after Removing Stop Words:\n",
      "['Stop', 'words', 'like', \"'the\", \"'\", \"'is\", \"'\", 'removed', 'better', 'analysis', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Stop words like 'the' and 'is' should be removed for better analysis.\"\n",
    "\n",
    "filtered_text = remove_stop_words(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nText after Removing Stop Words:\")\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ad1cd",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6273e1e",
   "metadata": {},
   "source": [
    "Process: Eliminating punctuation marks (e.g., commas, periods) from text during preprocessing.\n",
    "Benefits: Reduces noise, ensures uniformity, and aids in creating a cleaner text for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8c24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Text with punctuation, like commas, can affect NLP tasks.\n",
      "\n",
      "Text after Removing Punctuation:\n",
      "Text with punctuation like commas can affect NLP tasks\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Using string.punctuation to get a string of all punctuation marks\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punct = text.translate(translator)\n",
    "    return text_no_punct\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Text with punctuation, like commas, can affect NLP tasks.\"\n",
    "\n",
    "text_without_punct = remove_punctuation(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nText after Removing Punctuation:\")\n",
    "print(text_without_punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0edc64",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "757f287a",
   "metadata": {},
   "source": [
    "Importance: Ensures uniformity by converting all text to lowercase.\n",
    "Benefits: Treats words with different cases as the same, avoids redundancy, and enhances consistency in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8159e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Text with MiXeD CaSe for NLP analysis.\n",
      "\n",
      "Text after Lowercase Conversion:\n",
      "text with mixed case for nlp analysis.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Text with MiXeD CaSe for NLP analysis.\"\n",
    "\n",
    "lowercased_text = convert_to_lowercase(raw_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(raw_text)\n",
    "\n",
    "print(\"\\nText after Lowercase Conversion:\")\n",
    "print(lowercased_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8868195",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78869c64",
   "metadata": {},
   "source": [
    "Definition: Process of converting text data into numerical vectors.\n",
    "Purpose: Enables machine learning models to process and analyze textual information.\n",
    "CountVectorizer in Text Preprocessing:\n",
    "\n",
    "Definition: Converts a collection of text documents to a matrix of token counts.\n",
    "Contribution: Represents each document as a vector of word counts, creating a numerical format suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b14c97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Texts:\n",
      "['Text for NLP analysis.', 'Another text for vectorization.']\n",
      "\n",
      "Vectorized Matrix:\n",
      "[[1 0 1 1 1 0]\n",
      " [0 1 1 0 1 1]]\n",
      "\n",
      "Feature Names:\n",
      "['analysis' 'another' 'for' 'nlp' 'text' 'vectorization']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_text(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage\n",
    "corpus = [\"Text for NLP analysis.\", \"Another text for vectorization.\"]\n",
    "\n",
    "vectorized_matrix, feature_names = vectorize_text(corpus)\n",
    "\n",
    "print(\"Original Texts:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"\\nVectorized Matrix:\")\n",
    "print(vectorized_matrix.toarray())\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f4381",
   "metadata": {},
   "source": [
    "# 8 Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a972e42",
   "metadata": {},
   "source": [
    "Concept: Process of transforming text data to a consistent and standardized format.\n",
    "Purpose: Ensures uniformity, reduces redundancy, and aids in better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6793a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca2c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming:\n",
    "from nltk.stem import PorterStemmer\n",
    "def stemming(text):\n",
    "    porter = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [porter.stem(word) for word in tokens]\n",
    "    return stemmed_words\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb57449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b292756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
