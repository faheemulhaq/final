{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005f1f9c",
   "metadata": {},
   "source": [
    "# Build a NLP Language model to detect the sentence/word error in the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab2a43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c94921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\fahee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text :I woke up this morning feeling an overwhelming sense of joy. The sun streamed through the window, casting a warm glow that filled me with happiness. As I stepped outside, a gentle breeze brushed against my skin, evoking a sense of calm and contentment. However, as the day progressed, a wave of nostalgia washed over me, reminding me of cherished memories from the past. I found myself smiling at old photographs, feeling a mix of joy and longing.  Suddenly, a pang of sadness hit me as I remembered missed opportunities and lost connections. Yet, hope flickered within me, like a small flame refusing to be extinguished. Determination surged through my veins, propelling me forward despite the obstacles. Later, an unexpected surprise lifted my spirits, filling me with excitement and anticipation for what lay ahead. Ultimately, today has been a whirlwind of emotions, each one leaving its mark on my heart.\n",
      "\n",
      "Misspelled Words:\n",
      "['.', 'streamed', ',', '.', ',', ',', 'evoking', '.', ',', 'progressed', ',', ',', 'reminding', 'cherished', 'memories', '.', 'photographs', ',', '.', ',', 'remembered', 'missed', 'opportunities', 'connections', '.', ',', 'flickered', ',', '.', 'surged', 'veins', ',', 'propelling', 'obstacles', '.', ',', 'lifted', 'spirits', ',', '.', ',', 'has', 'emotions', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download the words corpus (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample text with intentional errors\n",
    "text = input(\"Enter the text :\")\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Get the set of English words from the nltk corpus\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "# Check for misspelled words\n",
    "misspelled_words = [word for word in tokens if word.lower() not in english_vocab]\n",
    "\n",
    "print()\n",
    "\n",
    "# Print misspelled words\n",
    "if len(misspelled_words) > 0:\n",
    "    print(\"Misspelled Words:\")\n",
    "    print(misspelled_words)\n",
    "else:\n",
    "    print(\"No misspelled words found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c631a",
   "metadata": {},
   "source": [
    "# Build a Language model to correct the error in the tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159a59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562aca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 5s 5s/step - loss: 3.0448 - accuracy: 0.0556\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0399 - accuracy: 0.1667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.0348 - accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.0297 - accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.0244 - accuracy: 0.3889\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.0189 - accuracy: 0.3889\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0131 - accuracy: 0.3889\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0069 - accuracy: 0.3889\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0003 - accuracy: 0.3889\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.9932 - accuracy: 0.3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ce36e183d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data - input sentences and their corrected versions\n",
    "sentences = [\n",
    "    \"I is going to the park.\",\n",
    "    \"He have a blue car.\",\n",
    "    \"She do not like ice cream.\"\n",
    "]\n",
    "\n",
    "corrected_sentences = [\n",
    "    \"I am going to the park.\",\n",
    "    \"He has a blue car.\",\n",
    "    \"She does not like ice cream.\"\n",
    "]\n",
    "\n",
    "# Tokenizing the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences + corrected_sentences)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "corrected_sequences = tokenizer.texts_to_sequences(corrected_sentences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(max(len(seq) for seq in sequences), max(len(seq) for seq in corrected_sequences))\n",
    "\n",
    "# Padding sequences to have uniform length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "padded_corrected_sequences = pad_sequences(corrected_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(padded_sequences, np.expand_dims(padded_corrected_sequences, -1), epochs=10, batch_size=32)\n",
    "\n",
    "# Now, you can use this trained model to correct sentences by predicting the corrected sequence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
